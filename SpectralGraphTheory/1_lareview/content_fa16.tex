\section{Linear Algebra Review: The Spectral Theorem}

In this lecture, we will review our most important tool for dealing with symmetric matrices: the spectral theorem. We start by recalling the idea of eigenvector and eigenvalue of a square matrix.

\begin{definition} % EIGENVECTOR
For a matrix $M \in \R^{n \times n}$ and a vector $x \in \R^n$, suppose that
$$
Mx = \lambda x.
$$
Then, we call  $\lambda$ an \textrm{eigenvalue} of $M$ and $x$ the associated \textrm{eigenvector}.
\end{definition}
In all our course, the underlying field will be the real numbers, so, unless specified otherwise, we will assume that the eigenvalues must be real.


\begin{theorem}[Spectral Theorem for Symmetric Matrices]
If $M$ is a symmetric $n\times n$ matrix, then $M$ has a full orthonormal basis of eigenvectors, i.e, $M$ can be written as
\begin{align}\label{eqn:eigendecomp}
M = V \Lambda V^T = \sum_{i=1}^{n} \lambda_i v_i v_i^T
\end{align}
where the columns $v_1, \ldots, v_n$ of $V$ are an orthonormal basis of eigenvectors and $\Lambda=\diag(\lambda_1, \ldots, \lambda_n)$ is a diagonal matrix of the corresponding eigenvalues.
\end{theorem}
Equation~\ref{eqn:eigendecomp} is called an {\it eigenvector decomposition} of $M$. You should familiarize yourself with both forms of the eigenvector decomposition. The form $V \Lambda V^T$ conveys the fact a change of basis to the basis $V$ makes the matrix $M$ diagonal. The other equivalent form is useful to visualize $M$ as a sum of projection onto the eigenvectors. This is the case as, for a unit vector $V$, the matrix $vv^T$ represent the linear operator that takes any vector $x$ to its projection  $(v^T x) v$ over $v.$


You should also notice what the spectral theorem does NOT imply. In particular, notice the following slightly subtle facts for a symmetric matrix $M$:
\begin{enumerate}
\item The matrix $M$ can have infinitely many eigenvectors. In particular, suppose that there are two orthogonal eigenvectors $v$ and $u$ with the same eigenvalue $\lambda.$ Then, for any vector $x$ in $\textrm{Span}(u,v)$, i.e., $x = a u + b v$ for $a,b \in \R$, we have:
$$
M x = M  (a u + b v) = a Mu + b Mv = a\lambda u + b \lambda v= \lambda x.
$$
Hence, any $x$ in the span of $u$ and $v$ is itself an eigenvector with eigenvalue $\lambda.$
More generally, an eigenvalue $\lambda$ may be associated with a subspace of eigenvectors of dimension $k.$ In this case, we say that $\lambda$ has multiplicity $k.$
\item The above example also shows that the eigenvector decomposition is not unique. In particular, when a degenerate eigenvalue is present, any orthonormal basis of its eigenspace can be used in the decomposition.
\end{enumerate}


To improve our grasp of the spectral theorem, we will actually spend the rest of the lecture proving it. The standard proof uses the characteristic polynomial and the Fundamental Theorem of Algebra, but for this class I want to give you a more algorithmic proof. I start by stating a claim that will be proved later.
\begin{claim} \label{claim}
Any non-zero symmetric $M$ has an eigenvector $x$ with non-zero eigenvalue $\lambda.$ 
\end{claim}
This claim is really a much weaker version of the spectral theorem. Rather than telling us that we have an orthonormal basis of eigenvectors, it tells us that one eigenvector exists. In the first part of the proof, we will show that this claim is sufficient to show the spectral theorem. In particular, we can inductively apply this claim to construct an eigenvector decomposition. More intuitively, we'll assume that we have a blackbox that, given a symmetric matrix, outputs an eigenvector and show an algorithm that on input $M$ queries the blackbox a number of times and constructs an eigenvector decomposition of $M$.

\begin{proof}[Proof of Spectral Theorem]
Consider the following iterative procedure. Notice that WLOG the eigenvector guaranteed by Claim~\ref{claim} can be rescaled to have norm $1$.
\begin{enumerate}
\item Starting with $M^{1} = M$ and $t=1,$ repeat the following as long as $M^{(t)} \neq 0.$
	\begin{enumerate}
    \item Use Claim~\ref{claim} to obtain a unit-norm eigenvector $x^{t}$ of $M^{(t)}$ together with its associated non-zero eigenvalue $\lambda_t.$
    \item Construct $M^{(t+1)}$, defined as $M^{(t+1)} = M^{(t)} - \lambda_t x^{(t)} {x^{(t)}}^T.$
    \item If $M^{(t+1)}$ = 0: Terminate. 
    \item Increment $t$ to $t+1.$
	\end{enumerate}
\end{enumerate}
Let us analyze this algorithm. To start with, it is easy to unravel the iterations to see that for all $t$:
$$
M^{(t+1)} = M^{(1)} - \sum_{j=1}^t \lambda_j x^{(j)} {x^{(j)}}^T = M - \sum_{j=1}^t \lambda_j x^{(j)} {x^{(j)}}^T. 
$$
We show that:
\begin{enumerate}
\item The vectors $(x^{(1)}, x^{(2)}, \ldots, x^{(t)})$ are eigenvectors of $M$ with non-zero eigenvalues $(\lambda_1, \lambda_2, \ldots, \lambda_t),$ and
\item The vectors $(x^{(1)}, x^{(2)}, \ldots, x^{(t)})$ are orthonormal.
\end{enumerate}
We do so by induction on $t.$ For $t=1,$ this is trivially true. Suppose the statement is true for $t=k.$ Then, for $i \leq k,$ we have:

$$
\lambda_{k+1}x^{(i)^T} x^{(k+1)} = x^{(i)^T} \left(\lambda_{k+1}x^{(k+1)}\right) = x^{(i)^T}M^{(k+1)}x^{(k+1)} = x^{(i)^T} \left(M - \sum_{j=1}^t \lambda_j x^{(j)}x^{(j)^T}\right)x^{(k+1)} = 0
$$


% Old proof:
%$$
%M^{(k+1)} x^{(i)} = (M - \sum_{j=1}^k \lambda_j x^{(j)} {x^{(j)}}^T)x^{(i)} = M x^{(i)} - \lambda_i x^{(i)} = 0.
%$$
%Moreover:
%$$
%\lambda_{k+1} x^{(k+1)} = M^{(k+1)} x^{(k+1)} =  (M - \sum_{j=1}^k \lambda_j x^{(j)} {x^{(j)}}^T) x^{(k+1)}
%$$
%Left-multiplying by $x^{(i)}$ and using the previous equality, we have:
%$$
%0 = \lambda_{k+1} {x^{(i)}}^T x^{(k+1)} = {x^{(i)}}^T M x^{(k+1)} - \sum_{j=1}^k \lambda_j ({x^{(i)}}^Tx^{(j)}) \cdot ({x^{(j)}}^T x^{(k+1)}) =  \lambda_i {x^{(i)}}^T x^{(k+1)}.
%$$
As $\lambda_i \neq 0$, this implies that $x^{(k+1)}$ is orthogonal to all the eigenvectors $(x^{(1)}, x^{(2)}, \ldots, x^{(t)})$, so that $(x^{(i)})_{i \in [k+1]}$ is an orthonormal set of vectors. It remains to show that $x^{(k+1)}$ is an eigenvector of $M$ with eigenvalue $\lambda_{k+1}$. This easily follows from the orthonormality, as:
$$
M x^{(k+1)} = (M - \sum_{j=1}^k \lambda_j x^{(j)} {x^{(j)}}^T) x^{(k+1)} = M^{(k+1)} x^{(k+1)} = \lambda_{k+1} x^{(k+1)}.
$$
This completes the proof by induction. To wrap up the whole proof, we need to argue that the process terminates after some number of rounds $T \leq n.$ To do, it suffices to notice that the orthonormal sequence $(x^{(1)}, x^{(2)}, \ldots, x^{(T)})$ can contain at most $n$ vectors as the dimension of the underlying space is $n.$
\end{proof}

We have effectively shown a reduction from computing an eigenvector decomposition to computing a single non-zero eigenvalue and associated eigenvector. To show that such an eigenvector exists, we need to prove the claim. For reason that we will explore later, once we discuss actual algorithms for computing eigenvectors, this proof has to be existential.

\begin{proof}[Proof of Claim~\ref{claim}]
Consider the function $f_M: \R^n \setminus\{0\}\to \R$.
$$
f_M(x) = \frac{x^T M x}{x^T x}.
$$
We will argue that the maximum of this function yields an eigenvalue of $M$, with the maximizer being the corresponding eigenvector. To prove this, notice that $f$ is continuous and differentiable over its domain, so that at every critical point $x_0$ we have:
$$
\nabla f_M(x_0) = \frac{(x_0^T x_0) \cdot 2M x_0 - (x_0^T M x_0) \cdot 2x_0}{(x_0^T x_0)^2} = 0.
$$
This implies that:
$$
M x_0 = \frac{x_0^T x_0}{x_0^T x_0} x_0 = f_M(x_0) x_0
$$

\textcolor{blue}{edit: $$
M x_0 = \frac{x_0^T M x_0}{x_0^T x_0} x_0 = f_M(x_0) x_0
$$}


showing that any critical point $x_0$ is an eigenvector of $M$ with eigenvalue $f_M(x_0).$
% ADD STEP BY STEP DERIVATION 

To show that a critical point exists, it suffices to show that $f_M$ achieves its maximum over its domain. For this, consider the restriction $\bar{f}_M$ of $f_M$ to the unit sphere $x^Tx=1.$
Because the fraction in $f_M$ is homogeneous in $x$, we have that $f_M(x) = \bar{f}_M(\frac{x}{\|x\|_2})$ for all $x \neq 0.$ Hence, it suffices to show that that $\bar{f}_M$ achieves its maximum.
To do so, notice that $\bar{f}_M(x) = x^T M x$ is a continuous function and that the domain $\{x^Tx = 1\}$ is compact, as it is closed and bounded. Then by the Bolzano-Weierstrass Extreme Value Theorem\footnote{A basic theorem in calculus, see \href{https://en.wikipedia.org/wiki/Extreme_value_theorem}{this Wikipedia page}.}, $\bar{f}_M$ achieves its maximum (and its minimum) over the domain and, hence, must have a critical point.
\end{proof}
\section{Exercises}
\begin{exercise}
Let $M$ be a symmetric $n\times n$ matrix. Use the spectral decomposition of $M$ to show that there exists an $n \times n$ symmetric matrix $M^{1/2}$ such that:
\[
	\left(M^{1/2}\right)^2 = M
\]
\end{exercise}